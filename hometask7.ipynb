{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ghK1srgpB1wf",
    "outputId": "a64b1478-d337-44eb-8413-841ea0f70888"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BSEchmRB1wh"
   },
   "source": [
    "Выведем список категорий текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "lR3knXluB1wi",
    "outputId": "d6b41c1a-21d8-4baf-de17-f6c8f7ce8f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "-eXBAq3SB1wy",
    "outputId": "37dca253-a58a-4390-af8f-43a8d751366d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'per', 'around', 'so', 'mill', 'except', 'enough', 'she', 'nobody', 'whole', 'somewhere', 'inc', 'beside', 'twelve', 'which', 'almost', 'you', 'elsewhere', 'whereupon', 'its', 'former', 'everywhere', 'although', 'there', 'be', 'were', 'anyhow', 'bottom', 'at', 'him', 'myself', ...however', 'as', 'next', 'become', 'fill', 'they', 'once', 'without', 'them', 'therefore', 'anyone'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZYK2sG_B1xn"
   },
   "source": [
    "Переведем весь набор текстов обучающего датасета в набор векторов, получим матрицу ```X_train```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uhj_aRW7B1xn",
    "outputId": "76e48a96-b2ef-4512-a807-026daf579ece"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9ZpBC7Sgauk"
   },
   "source": [
    "\n",
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 20\n",
    "nk=np.zeros(tag)\n",
    "ndk=np.zeros(tag*X_train.shape[0]).reshape(X_train.shape[0],tag)\n",
    "nkw=np.zeros(tag*X_train.shape[1]).reshape(tag,X_train.shape[1])\n",
    "doc, word = X_train.nonzero()\n",
    "z = np.random.choice(tag, len(doc))\n",
    "for i, j, k in zip(doc, word, z):\n",
    "    ndk[i, k] += 1\n",
    "    nkw[k, j] += 1\n",
    "    nk[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rn_580PkLCCj"
   },
   "outputs": [],
   "source": [
    "def lda(ndk, nkw, nk, z, doc, word, tag, a1, a2, it):    \n",
    "    for i in tqdm(range(it)):\n",
    "        for j in range(len(doc)):\n",
    "            cword = word[j]\n",
    "            cdoc = doc[j]\n",
    "            ctag = z[j]\n",
    "            ndk[cdoc, ctag] -= 1\n",
    "            nkw[ctag, cword] -= 1\n",
    "            nk[ctag] -= 1\n",
    "            p = (ndk[cdoc, :] + a1) * (nkw[:, cword] + a2[cword]) / (nk + a2.sum())\n",
    "            z[j] = np.random.choice(np.arange(tag), p=p / p.sum())\n",
    "            ndk[cdoc, z[j]] += 1\n",
    "            nkw[z[j], cword] += 1\n",
    "            nk[z[j]] += 1\n",
    "    return z, nkw, ndk, nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [24:55<00:00, 30.27s/it]\n"
     ]
    }
   ],
   "source": [
    "z,nkw,ndk,nk=lda(ndk, nkw, nk, z, doc, word, 20, 2*np.ones(20), 2*np.ones(X_train.shape[1]), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 car\tdoing\tgetting\theard\tline\tlist\tlots\tmoney\tquite\treply\n",
      "2 bible\tchristian\tchristians\tclaim\tjesus\tlife\tman\tmean\treason\tsays\n",
      "3 bad\tbig\tcar\tcause\tdeal\tdoing\theard\tproblems\twrong\tyes\n",
      "4 card\tcomputer\tdisk\tdrive\thi\tmemory\tpc\tsale\tsoftware\tvideo\n",
      "5 1993\tapril\tearth\tlow\tnational\tprogram\tresearch\tscience\tspace\tuniversity\n",
      "6 article\tcar\tdifference\tlarge\tleft\tlist\tremember\tseen\ttrying\twon\n",
      "7 chip\tclipper\tencryption\tgovernment\tkey\tkeys\tlaw\tphone\tprivate\tpublic\n",
      "8 couple\tfriend\tgame\tgetting\tmakes\tsimple\tsource\tstuff\tunless\twanted\n",
      "9 11\t14\t17\t20\t30\tgame\tgames\tplay\tseason\tteam\n",
      "10 ago\tanybody\tbad\tchange\tgoes\toh\tremember\tstuff\twon\tyes\n",
      "11 banks\tgeb\tgordon\tintellect\tisn\tpitt\tshameful\tskepticism\tsoon\tsurrender\n",
      "12 ago\tcouple\tfree\tgetting\theard\tlooked\tpretty\tremember\tsimilar\tthought\n",
      "13 application\tcode\tfile\tfiles\tftp\tprogram\tsend\tversion\twindow\twrite\n",
      "14 big\tgetting\tjob\tkind\tline\tposted\tquite\treading\treason\ttimes\n",
      "15 big\tidea\tlevel\tlooks\tmind\tnote\tproblems\tseen\tsorry\tunless\n",
      "16 ago\tanybody\tbig\tguy\tmoney\tpay\tproblems\tstuff\twon\twrong\n",
      "17 bad\tcar\tfine\tguess\tpretty\tsort\tstart\twon\tworth\tyes\n",
      "18 away\tcar\tcondition\tfree\tline\torder\tproblems\treply\tsmall\tstate\n",
      "19 away\tchildren\tcountry\tgovernment\tgun\tisrael\tkilled\tstate\tstates\twar\n",
      "20 area\tgetting\toh\tpretty\tquite\trunning\tseen\tsorry\tthought\ttried\n"
     ]
    }
   ],
   "source": [
    "A = np.argsort(nkw, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, X_train.shape[1]))\n",
    "    for j in A[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print(i+1,'\\t'.join(vectorizer.inverse_transform(matrix)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем соотнести полученные результаты со списком категорий текстов:      \n",
    " 'alt.atheism' - 11       \n",
    " 'comp.graphics' - 4,13,15      \n",
    " 'comp.os.ms-windows.misc' - 4,13,15      \n",
    " 'comp.sys.ibm.pc.hardware' - 4,13,15      \n",
    " 'comp.sys.mac.hardware' - 4,13,15      \n",
    " 'comp.windows.x' - 4,13,15     \n",
    " 'misc.forsale' - 4     \n",
    " 'rec.autos' - 1,3,6,17,18      \n",
    " 'rec.motorcycles' - 1,3,6,17,18      \n",
    " 'rec.sport.baseball' - 9      \n",
    " 'rec.sport.hockey' - 9     \n",
    " 'sci.crypt' - 7     \n",
    " 'sci.electronics' - 4      \n",
    " 'sci.med'      \n",
    " 'sci.space' - 5      \n",
    " 'soc.religion.christian' - 2      \n",
    " 'talk.politics.guns' - 19      \n",
    " 'talk.politics.mideast' - 19     \n",
    " 'talk.politics.misc'       \n",
    " 'talk.religion.misc'     \n",
    " К некоторым категория нельзя однозначно подобрать тэги, как и к некоторым тэгам нельзя однозначно подобрать категории. Также можно сказать о том, что к некоторым категориям невозможно подобрать тэги, потому что нет слов специализирующихся именно в той или другой категории. Но также можно заметить, что большая часть тэгов разобралась по категориям."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Topic Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
